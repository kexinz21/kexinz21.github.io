<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - Kexin Zhang</title>
    <!-- Include Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <!-- Include Inter font -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
    <!-- Link to external CSS -->
    <link rel="stylesheet" href="styles.css">
</head>

<header>
    <div class="brand-name">
      <a href="index.html">Kexin Zhang</a>
    </div>
    <nav>
      <ul class="nav-links">
        <li><a href="index.html">About</a></li>
        <li><a href="publications.html">Publications</a></li>
        <!-- <li><a href="projects.html">Projects</a></li> -->
        <li><a href="https://drive.google.com/file/d/1nHeeJL_4QJApXP5OLRVW9c39LmVgAmgP/view?usp=sharing" target="_blank">CV</a></li>
      </ul>
    </nav>
  </header>

    <main>
        <section id="publications" class="publications">
        <h4>Find me on <a href="https://scholar.google.com/citations?user=RNoZyNoAAAAJ&hl=en" target="_blank">Google Scholar. </a> </h4>
            <!-- CHI 24 -->
            
            <div class="publication">
                <img src="https://raw.githubusercontent.com/kexinz21/kexinz21.github.io/main/images/CHI'25.png" alt="chi25">
                <div>
                    <div class="publication-details">
                        <h3>Inclusive Avatar Guidelines for People with Disabilities: Supporting Disability Representation in Social Virtual Reality</h3>
                        <p><i>CHI 2025</i></p>
                        <p class="authors"><b>Kexin Zhang</b>, Edward Glenn Scott Spencer, Abijith Manikandan, Andric Li, Ang Li, Yaxing Yao, Yuhang Zhao.</p>
                        <div class="publication-buttons">
                            <a href="#" class="abs-button">abstract</a>
                            <a href="https://dl.acm.org/doi/full/10.1145/3706598.3714230" target="_blank">doi</a>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3706598.3714230" target="_blank">pdf</a>
                            <a href="https://github.com/MadisonAbilityLab/Inclusive-Avatar-Guidelines-and-Library" target="_blank">GitHub</a>
                        </div>

                    </div>
                    <div class="abstract">
                        Avatar is a critical medium for identity representation in social virtual reality (VR). However, options for disability expression are highly limited on current avatar interfaces. Improperly designed disability features may even perpetuate misconceptions about people with disabilities (PWD). As more PWD use social VR, there is an emerging need for comprehensive design standards that guide developers and designers to create inclusive avatars. Our work aim to advance the avatar design practices by delivering a set of centralized, comprehensive, and validated design guidelines that are easy to adopt, disseminate, and update. Through a systematic literature review and interview with 60 participants with various disabilities, we derived 20 initial design guidelines that cover diverse disability expression methods through five aspects, including avatar appearance, body dynamics, assistive technology design, peripherals around avatars, and customization control. We further evaluated the guidelines via a heuristic evaluation study with 10 VR practitioners, validating the guideline coverage, applicability, and actionability. Our evaluation resulted in a final set of 17 design guidelines with recommendation levels.
                    </div>
                </div>
                <!-- Add time bar here -->
                <div class="time-bar">2025</div>
            </div>
            
            <div class="publication">
                <img src="https://raw.githubusercontent.com/kexinz21/kexinz21.github.io/main/images/chi'24.png" alt="chi24">
                <div>
                    <div class="publication-details">
                        <h3>Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field</h3>
                        <p><i>CHI 2024</i></p>
                        <p class="authors"><b>Kexin Zhang</b>, Brianna Cochran, Ruijia Chen, Lance Hartung, Bryce Sprecher, Ross Tredinnick, Kevin Ponto, Suman Banerjee, Yuhang Zhao.</p>
                        <div class="publication-buttons">
                            <a href="#" class="abs-button">abstract</a>
                            <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642195" target="_blank">doi</a>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3613904.3642195" target="_blank">pdf</a>
                        </div>

                    </div>
                    <div class="abstract">
                        First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs.
                    </div>
                </div>
                <!-- Add time bar here -->
                <div class="time-bar">2024</div>
            </div>

            <!-- ASSETS '24 -->
            <div class="publication">
                <img src="https://raw.githubusercontent.com/kexinz21/kexinz21.github.io/main/images/assets'24.png" alt="assets24">
                <div>
                    <div class="publication-details">
                        <h3>“I Try to Represent Myself as I Am”: Self-Presentation Preferences of People with Invisible Disabilities through Embodied Social VR Avatars</h3>
                        <p><i>ASSETS 2024</i></p>
                        <p class="authors">Ria J. Gualano*, Lucy Jiang*, <b>Kexin Zhang</b>, Tanisha Shende, Andrea Stevenson Won, Shiri Azenkot</p>
                        <div class="publication-buttons">
                            <a href="#" class="abs-button">abstract</a>
                            <a href="https://dl.acm.org/doi/10.1145/3663548.3675620" target="_blank">doi</a>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3663548.3675620" target="_blank">pdf</a>
                        </div>
                    </div>
                    <div class="abstract">
                        With the increasing adoption of social virtual reality (VR), it is critical to design inclusive avatars. While researchers have investigated how and why blind and d/Deaf people wish to disclose their disabilities in VR, little is known about the preferences of many others with invisible disabilities (e.g., ADHD, dyslexia, chronic conditions). We filled this gap by interviewing 15 participants, each with one to three invisible disabilities, who represented 22 different invisible disabilities in total. We found that invisibly disabled people approached avatar-based disclosure through contextualized considerations informed by their prior experiences. For example, some wished to use VR’s embodied affordances, such as facial expressions and body language, to dynamically represent their energy level or willingness to engage with others, while others preferred not to disclose their disability identity in any context. We define a binary framework for embodied invisible disability expression (public and private) and discuss three disclosure patterns (Activists, Non-Disclosers, and Situational Disclosers) to inform the design of future inclusive VR experiences.
                    </div>
                </div>
            </div>

            <!-- ASSETS '23 -->
            <div class="publication">
                <img src="https://raw.githubusercontent.com/kexinz21/kexinz21.github.io/main/images/assets'23.png" alt="assets23">
                <div>
                    <div class="publication-details">
                        <h3>A diary study in social virtual reality: Impact of avatars with disability signifiers on the social experiences of people with disabilities</h3>
                        <p><i> ASSETS 2023</i></p>
                        <p class="authors"><b>Kexin Zhang</b>, Elmira Deldari, Yaxing Yao, and Yuhang Zhao.</p>
                        <div class="publication-buttons">
                            <a href="#" class="abs-button">abstract</a>
                            <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388" target="_blank">doi</a>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3597638.3608388" target="_blank">pdf</a>
                        </div>
                    </div>
                    <div class="abstract">
                        People with disabilities (PWD) have shown a growing presence in the emerging social virtual reality (VR). To support disability representation, some social VR platforms start to involve disability features in avatar design. However, it is unclear how disability disclosure via avatars (and the way to present it) would affect PWD’s social experiences and interaction dynamics with others. To fill this gap, we conducted a diary study with 10 PWD who freely explored VRChat—a popular commercial social VR platform—for two weeks, comparing their experiences between using regular avatars and avatars with disability signifiers (i.e., avatar features that indicate the user’s disability in real life). We found that PWD preferred using avatars with disability signifiers and wanted to further enhance their aesthetics and interactivity. However, such avatars also caused embodied, explicit harassment targeting PWD. We revealed the unique factors that led to such harassment and derived design implications and protection mechanisms to inspire more safe and inclusive social VR.
                    </div>
                </div>
                <div class="time-bar">2023</div>
            </div>

            <!-- ASSETS '23 Poster -->
            <div class="publication">
                <img src="https://raw.githubusercontent.com/kexinz21/kexinz21.github.io/main/images/23_poster.png" alt="assets23_poster">
                <div>
                    <div class="publication-details">
                        <h3>“Invisible Illness Is No Longer Invisible”: Making Social VR Avatars More Inclusive for Invisible Disability Representation.</h3>
                        <p><i>ASSETS 2023 Poster</i></p>
                        <p class="authors">Ria J Gualano*, Lucy Jiang*, <b>Kexin Zhang*</b>, Andrea Stevenson Won, Shiri Azenkot.</p>
                        <div class="publication-buttons">
                            <a href="#" class="abs-button">abstract</a>
                            <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3614480" target="_blank">doi</a>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3597638.3614480" target="_blank">pdf</a>
                        </div>
                    </div>
                    <div class="abstract">
                        As social virtual reality (VR) experiences become more popular, it is critical to design accessible and inclusive embodied avatars. At present, there are few, if any, customization features for invisible disabilities (e.g., chronic health conditions, mental health conditions, neurodivergence) in social VR platforms. To our knowledge, researchers have yet to explore how people with invisible disabilities want to self-represent and disclose disabilities through social VR avatars. We fill this gap in current accessibility research by centering the experiences and preferences of people with invisible disabilities. We conducted semi-structured interviews with nine participants and found that people with invisible disabilities used a unique, indirect approach to inform dynamic disclosure practices. Participants were interested in toggling representation on/off across contexts and shared ideas for representation through avatar design. In addition, they proposed ways to make the customization process more accessible (e.g., making it easier to import custom designs). We see our work as a vital contribution to the growing literature that calls for more inclusive social VR.
                    </div>
                </div>
            </div>

            <!-- ASSETS '22 -->
            <div class="publication">
                <img src="https://raw.githubusercontent.com/kexinz21/kexinz21.github.io/main/images/assets'22.png" alt="assets22">
                <div>
                    <div class="publication-details">
                        <h3>“It’s Just Part of Me:” Understanding Avatar Diversity and Self-presentation of People with Disabilities in Social Virtual Reality</h3>
                        <p><i>ASSETS 2022</i></p>
                        <p class="authors"><b>Kexin Zhang</b>, Elmira Deldari, Zhicong Lu, Yaxing Yao, and Yuhang Zhao.</p>
                        <div class="publication-buttons">
                            <a href="#" class="abs-button">abstract</a>
                            <a href="https://dl.acm.org/doi/10.1145/3517428.3544829" target="_blank">doi</a>
                            <a href="https://dl.acm.org/doi/pdf/10.1145/3517428.3544829" target="_blank">pdf</a>
                        </div>
                    </div>
                    <div class="abstract">
                        In social Virtual Reality (VR), users are embodied in avatars and interact with other users in a face-to-face manner using avatars as the medium. With the advent of social VR, people with disabilities (PWD) have shown an increasing presence on this new social media. With their unique disability identity, it is not clear how PWD perceive their avatars and whether and how they prefer to disclose their disability when presenting themselves in social VR. We fill this gap by exploring PWD’s avatar perception and disability disclosure preferences in social VR. Our study involved two steps. We first conducted a systematic review of fifteen popular social VR applications to evaluate their avatar diversity and accessibility support. We then conducted an in-depth interview study with 19 participants who had different disabilities to understand their avatar experiences. Our research revealed a number of disability disclosure preferences and strategies adopted by PWD (e.g., reflect selective disabilities, present a capable self). We also identified several challenges faced by PWD during their avatar customization process. We discuss the design implications to promote avatar accessibility and diversity for future social VR platforms.
                    </div>
                </div>
                <!-- Add time bar here -->
                <div class="time-bar">2022</div>
            </div>
        </section>
        <!-- Updated content ends here -->
    </main>

    <footer>
        <p>&copy; 2024 Kexin Zhang. All rights reserved.</p>
    </footer>

    <!-- Include JavaScript for toggling abstracts -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const absButtons = document.querySelectorAll('.abs-button');
            absButtons.forEach(button => {
                button.addEventListener('click', function (event) {
                    event.preventDefault();
                    const abstract = this.closest('.publication-details').nextElementSibling;
                    if (abstract.style.display === 'none' || abstract.style.display === '') {
                        abstract.style.display = 'block';
                    } else {
                        abstract.style.display = 'none';
                    }
                });
            });
        });
    </script>

    <script>
        function toggleBibtex(button) {
            var bibtexEntry = button.parentElement.nextElementSibling;
            if (bibtexEntry.style.display === "none") {
                bibtexEntry.style.display = "block";
            } else {
                bibtexEntry.style.display = "none";
            }
        }
    </script>
</html>
