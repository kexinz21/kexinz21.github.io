<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kexin Zhang | PhD Student in HCI</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <!-- Include Font Awesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        :root {
            --bg-color: #ffffff;
            --text-main: #1d1d1f;
            --text-muted: #6e6e73;
            --accent: #0066cc;
            --section-bg: #f5f5f7;
            --border-color: #d2d2d7;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: var(--text-main);
            margin: 0;
            padding: 0;
            background-color: var(--bg-color);
            -webkit-font-smoothing: antialiased;
        }

        /* Navigation header */
        header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            padding: 20px 10px;
            max-width: 900px;
            margin: auto;
            position: sticky;
            top: 0;
            background-color: #ffffff;
            z-index: 1000;
        }
        
        .brand-name {
            font-size: 2.1em;
            font-weight: 600;
            margin: 0;
        }
        
        .brand-name a {
            text-decoration: none;
            color: #000;
        }
        
        .brand-name a:visited {
            color: #000;
        }
        
        .brand-name a:hover {
            color: #3f3f3f;
        }
        
        .nav-links {
            display: flex;
            list-style: none;
            margin: 0;
            padding: 20px;
        }
        
        .nav-links li {
            margin-left: 20px;
        }
        
        .nav-links li:first-child {
            margin-left: 0;
        }
        
        .nav-links li a {
            text-decoration: none;
            color: #040404;
            font-size: 1.2em;
            transition: color 0.3s;
            position: relative;
            padding-bottom: 4px;
        }
        
        .nav-links li a::after {
            content: "";
            position: absolute;
            width: 100%;
            height: 2px;
            background-color: #616161;
            bottom: 0;
            left: 0;
            transform: scaleX(0);
            transform-origin: bottom left;
            transition: transform 0.3s ease;
        }
        
        .nav-links li a:hover::after {
            transform: scaleX(1);
        }
        
        .nav-links li a:hover {
            color: #616161;
        }
        
        .about-me {
            display: flex;
            flex-direction: row;
            align-items: flex-start;
            justify-content: flex-start;
            gap: 50px;
            padding-top: 10px;
            padding-bottom: 10px;
            max-width: 900px;
            margin: 0 auto;
            margin-bottom: 10px;
        }
        
        section#about {
            margin-bottom: 0 !important;
        }
        
        section#news {
            margin-top: 0 !important;
            margin-bottom: 60px;
        }
        
        section#news h2 {
            margin-top: 20px;
        }

        .left-column {
            display: flex;
            flex-direction: column;
            align-items: center;
            width: 260px;
            flex-shrink: 0;
        }
        
        .profile-photo {
            margin: 0;
        }

        .profile-photo {
            width: 260px;
            height: 270px;
            background-color: var(--section-bg);
            border-radius: 0;
            overflow: hidden;
            border: 1px solid rgba(0,0,0,0.05);
        }

        .profile-photo img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .profile-info {
            text-align: center;
            margin-top: 15px;
            font-size: 0.95rem;
            color: var(--text-muted);
            line-height: 1.5;
        }

        .profile-info div:first-child {
            font-weight: 600;
            color: var(--text-main);
        }

        .social-links {
            margin-top: 20px;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
        }

        .social-links-row {
            display: flex;
            flex-wrap: nowrap;
            justify-content: center;
            align-items: center;
            gap: 0;
        }
        
        .social-links-row a:not(:last-child)::after {
            content: " | ";
            color: #999;
            margin-left: 10px;
            margin-right: 10px;
            font-weight: 400;
        }

        .social-links a {
            text-decoration: none;
            color: var(--accent);
            font-weight: 600;
            font-size: 0.95rem;
            transition: opacity 0.2s;
        }

        .email-text {
            color: var(--text-main);
            font-weight: 400;
            font-size: 0.95rem;
        }
        
        .email-text strong {
            font-weight: 600;
        }

        .email-text a:hover {
            opacity: 0.7;
        }

        .social-links a:hover {
            opacity: 0.7;
        }

        .about-text {
            flex: 1;
            font-size: 1.05rem;
            line-height: 1.7;
            margin-top: 0;
            padding-top: 0;
        }
        
        .about-text p:first-child {
            margin-top: 0;
        }

        .about-text a {
            color: var(--accent);
            text-decoration: none;
        }

        .about-text a:hover {
            text-decoration: underline;
        }

        main {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }

        section {
            margin-bottom: 60px;
        }

        h2 {
            font-size: 1.7rem;
            font-weight: 600;
            letter-spacing: -0.02em;
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 12px;
            margin-bottom: 30px;
        }
        
        section#news h2 {
            border-bottom: none;
            padding-bottom: 0;
        }


        /* Consistent News Box */
        .news-box {
            background: var(--section-bg);
            padding: 30px;
            border-radius: 12px;
            max-height: 140px;
            overflow-y: auto;
        }

        .news-item {
            margin-bottom: 15px;
            display: flex;
            font-size: 1rem;
        }

        .news-item:last-child {
            margin-bottom: 0;
        }

        .news-date {
            font-weight: 700;
            color: var(--text-main);
            min-width: 120px;
        }

        .news-content {
            color: var(--text-muted);
        }
        
        .news-content a {
            color: var(--text-muted);
            text-decoration: none;
        }
        
        .news-content a:hover {
            text-decoration: underline;
        }

        /* Selected Publications */
        .pub-item {
            margin-bottom: 30px;
            padding-bottom: 30px;
            border-bottom: 1px solid var(--border-color);
            display: flex;
            gap: 20px;
            align-items: flex-start;
        }
        
        .pub-item-content {
            flex: 1;
        }
        
        .pub-image {
            width: 265px;
            height: 165px;
            object-fit: cover;
            border-radius: 4px;
            flex-shrink: 0;
        }

        .pub-item:last-child {
            border-bottom: none;
            margin-bottom: 0;
            padding-bottom: 0;
        }

        .pub-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text-main);
            margin-bottom: 8px;
            line-height: 1.4;
        }

        .pub-title a {
            color: var(--text-main);
            text-decoration: none;
        }

        .pub-title a:hover {
            color: var(--accent);
        }

        .pub-venue {
            font-size: 1rem;
            font-style: italic;
            color: var(--text-muted);
            margin-bottom: 6px;
        }

        .pub-authors {
            font-size: 0.95rem;
            color: var(--text-muted);
            margin-bottom: 8px;
        }
        
        .pub-authors b {
            color: var(--text-main);
            font-weight: 600;
        }

        .pub-links {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
        }

        .pub-links a {
            font-size: 0.9rem;
            color: var(--accent);
            text-decoration: none;
            font-weight: 500;
        }

        .pub-links a:hover {
            text-decoration: underline;
        }
        
        .pub-abstract {
            display: none;
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid var(--border-color);
            font-size: 0.95rem;
            line-height: 1.6;
            color: var(--text-muted);
        }
        
        .pub-abstract strong {
            color: var(--text-main);
            font-weight: 600;
        }
        
        .pub-abstract.show {
            display: block;
        }
        
        .abs-button {
            cursor: pointer;
        }

        footer {
            text-align: center;
            padding: 80px 40px;
            font-size: 0.9rem;
            color: #b2bec3;
        }

        @media (max-width: 768px) {
            .about-me {
                flex-direction: column;
                align-items: center;
            }
            
            .left-column {
                width: 100%;
            }
        }
    </style>
</head>
<body>

<header>
    <div class="brand-name">
      <a href="index.html">Kexin Zhang</a>
    </div>
    <nav>
      <ul class="nav-links">
        <li><a href="#about" class="nav-link">About</a></li>
        <li><a href="#publications" class="nav-link">Publications</a></li>
        <li><a href="https://drive.google.com/file/d/1DAVx5UBFpij2AaOlg-z-3_1rVTnhQO2Z/view" target="_blank">CV</a></li>
      </ul>
    </nav>
</header>

<main>
    <section id="about" class="about-me">
        <div class="left-column">
            <div class="profile-photo">
                <img src="images/website_photo.jpg" alt="Kexin Zhang">
            </div>
            <div class="profile-info">
                <div>PhD Student</div>
                <div>Department of Computer Sciences</div>
                <div>University of Wisconsin-Madison</div>
            </div>
            <div class="social-links">
                <div class="email-text"><strong>Email:</strong> kzhang284 [at] wisc.edu</a></div>
                <div class="social-links-row">
                    <a href="https://drive.google.com/file/d/1DAVx5UBFpij2AaOlg-z-3_1rVTnhQO2Z/view" target="_blank">CV</a>
                    <a href="https://scholar.google.com/citations?user=RNoZyNoAAAAJ&hl=en" target="_blank">Google Scholar</a>
                    <a href="https://www.linkedin.com/in/kexin-zhang0201/" target="_blank">LinkedIn</a>
                </div>
            </div>
        </div>
        <div class="about-text">
            <p> I am a third-year PhD student in Computer Science at the <a href="https://cdis.wisc.edu/" target="_blank">University of Wisconsin-Madison</a>, advised by Dr. <a href="https://yuhangz.com" target="_blank"> Yuhang Zhao</a>. 
                    My research interests are in Human-Computer Interaction (HCI) and accessibility.
                    Broadly, my work aims to advance accessibility, inclusion, and fairness of emerging technologies through sociotechnical approaches. 
            </p>
            <p>
                My recent work focused on shaping prosocial behaviors on social virtual reality for people with disabilities in two key directions: 
                1) supporting inclusive and authentic disability representation in embodied avatars by developing design guidelines 
                and developer toolkit, and 2) designing and building protection mechanisms to mitigate ableist harassment and harm.
            </p>
            <p>Previously, I received a MS in Information Science from Cornell University, 
                where I worked with Dr. <a href="https://cals.cornell.edu/andrea-stevenson-won" target="_blank"> Andrea Stevenson Won</a> and Dr.<a href="https://infosci.cornell.edu/content/azenkot" target="_blank"> Shiri Azenkot</a>. 
                Before that, I earned my BA with a double major in Sociology and Economics from the University of Wisconsin-Madison.
            </p>
        </div>
    </section>

    <section id="news">
        <h2>News</h2>
        <div class="news-box">
            <div class="news-item">
                <span class="news-date">Oct. 2025</span>
                <span class="news-content">ðŸŽ¤Invited to give a talk at <em>Disability in STEM Symposium</em> about my research on supporting inclusive disability representation in social VR.</span>
            </div>
            <div class="news-item">
                <span class="news-date">Jun. 2025</span>
                <span class="news-content">ðŸŽ‰One co-authored paper on <a href="https://dl.acm.org/doi/full/10.1145/3663547.3746394" target="_blank">understanding OCD individual's real-world challenges</a> is accepted to ASSETS' 25!</span>
            </div>
            <div class="news-item">
                <span class="news-date">May 2025</span>
                <span class="news-content">Attended and student voluntered at <a href="https://chi2025.acm.org/" target="_blank">CHI 2025</a> in Yokohama, JapanðŸŒ¸</span>
            </div>
            <div class="news-item">
                <span class="news-date">Jan. 2025</span>
                <span class="news-content">ðŸŽ‰My paper on <a href="https://dl.acm.org/doi/full/10.1145/3706598.3714230" target="_blank">Inclusive Avatar Guidelines</a> is accepted to CHI 2025!</span>
            </div>
            <div class="news-item">
                <span class="news-date">May 2024</span>
                <span class="news-content">Presented my paper at <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642195" target="_blank">CHI 2024</a> in Honolulu, HawaiiðŸŒº</span>
            </div>
            <div class="news-item">
                <span class="news-date">Oct. 2023</span>
                <span class="news-content">Traveled to New York City to present my paper on <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388" target="_blank">examining ableist harassment in social VR</a> at ASSETS 2023ðŸ—½</span>
            </div>
            <div class="news-item">
                <span class="news-date">Jun 2023</span>
                <span class="news-content">Selected to attend <a href="https://hcic.org/" target="_blank">HCIC</a> in Delavan WI!</span>
            </div>
            <div class="news-item">
                <span class="news-date">May 2023</span>
                <span class="news-content">ðŸŽ“Started my PhD at University of Wisconsin-Madison, excited to get back to Madison after a wonderful year at Cornell.</span>
            </div>
        </div>
    </section>

    <section id="publications">
        <h2>Publications</h2>
        <div class="pub-item">
            <img src="images/CHI'25.png" alt="CHI 2025" class="pub-image">
            <div class="pub-item-content">
                <div class="pub-title">
                    <a href="https://dl.acm.org/doi/full/10.1145/3706598.3714230" target="_blank">Inclusive Avatar Guidelines for People with Disabilities: Supporting Disability Representation in Social Virtual Reality</a>
                </div>
                <div class="pub-authors"><b>Kexin Zhang</b>, Edward Glenn Scott Spencer, Abijith Manikandan, Andric Li, Ang Li, Yaxing Yao, Yuhang Zhao.</div>
                <div class="pub-venue">CHI 2025</div>
                <div class="pub-links">
                    <a href="#" class="abs-button">ABS</a>
                    <a href="https://dl.acm.org/doi/full/10.1145/3706598.3714230" target="_blank">DOI</a>
                    <a href="https://dl.acm.org/doi/pdf/10.1145/3706598.3714230" target="_blank">PDF</a>
                    <a href="https://github.com/MadisonAbilityLab/Inclusive-Avatar-Guidelines-and-Library" target="_blank">GitHub</a>
                </div>
                <div class="pub-abstract">
                    Avatar is a critical medium for identity representation in social virtual reality (VR). 
                    However, options for disability expression are highly limited on current avatar interfaces. 
                    Improperly designed disability features may even perpetuate misconceptions about people with disabilities (PWD). 
                    As more PWD use social VR, there is an emerging need for comprehensive design standards that guide developers and designers to create inclusive avatars. 
                    <strong>Our work aim to advance the avatar design practices by delivering a set of centralized, comprehensive, and validated design guidelines that are easy to adopt, disseminate, and update.</strong> 
                    Through a systematic literature review and interview with 60 participants with various disabilities, we derived 20 initial design guidelines that cover diverse disability expression methods through five aspects, including avatar appearance, body dynamics, assistive technology design, peripherals around avatars, and customization control. We further evaluated the guidelines via a heuristic evaluation study with 10 VR practitioners, validating the guideline coverage, applicability, and actionability. Our evaluation resulted in a final set of 17 design guidelines with recommendation levels.
                </div>
            </div>
        </div>

        <div class="pub-item">
            <img src="images/ASSETS '25.png" alt="ASSETS 2025" class="pub-image">
            <div class="pub-item-content">
                <div class="pub-title">
                    <a href="https://dl.acm.org/doi/full/10.1145/3663547.3746394" target="_blank">"It was Mentally Painful to Try and Stop": Design Opportunities for Just-in-Time Interventions for People with Obsessive-Compulsive Disorder in the Real World</a>
                </div>
                <div class="pub-authors">Ru wang, <b>Kexin Zhang</b>, Yuqing Wang, Keri Brown, Yuhang Zhao</div>
                <div class="pub-venue">ASSETS 2025</div>
                <div class="pub-links">
                    <a href="#" class="abs-button">ABS</a>
                    <a href="https://dl.acm.org/doi/full/10.1145/3663547.3746394" target="_blank">DOI</a>
                    <a href="https://dl.acm.org/doi/epdf/10.1145/3663547.3746394" target="_blank">PDF</a>
                </div>
                <div class="pub-abstract">
                    Obsessive-compulsive disorder (OCD) is a mental health condition that significantly impacts people's quality of life. While evidence-based therapies such as exposure and response prevention (ERP) can be effective, managing OCD symptoms in everyday lifeâ€”an essential part of treatment and independent livingâ€”remains challenging due to fear confrontation and lack of appropriate support. To better understand the challenges and needs in OCD self-management, we conducted interviews with 10 participants with diverse OCD conditions and seven therapists specializing in OCD treatment. Through these interviews, we explored the characteristics of participants' triggers and how they shaped their compulsions, and uncovered key coping strategies across different stages of OCD episodes. Our findings highlight critical gaps between OCD self-management needs and currently available support. Building on these insights, we propose design opportunities for just-in-time self-management technologies for OCD, including personalized symptom tracking, just-in-time interventions, and support for OCD-specific privacy and social needsâ€”through technology and beyond.
                </div>
            </div>
        </div>

        <div class="pub-item">
            <img src="images/chi24.png" alt="CHI 2024" class="pub-image">
            <div class="pub-item-content">
                <div class="pub-title">
                    <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642195" target="_blank">Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field</a>
            </div>
                <div class="pub-authors"><b>Kexin Zhang</b>, Brianna Cochran, Ruijia Chen, Lance Hartung, Bryce Sprecher, Ross Tredinnick, Kevin Ponto, Suman Banerjee, Yuhang Zhao.</div>
                <div class="pub-venue">CHI 2024</div>
                <div class="pub-links">
                    <a href="#" class="abs-button">ABS</a>
                    <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642195" target="_blank">DOI</a>
                    <a href="https://dl.acm.org/doi/epdf/10.1145/3613904.3642195" target="_blank">PDF</a>
                </div>
                <div class="pub-abstract">
                    First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs.
                </div>
            </div>
        </div>

        <div class="pub-item">
            <img src="images/assets24.png" alt="ASSETS 2024" class="pub-image">
            <div class="pub-item-content">
                <div class="pub-title">
                    <a href="https://dl.acm.org/doi/10.1145/3663548.3675620" target="_blank">"I Try to Represent Myself as I Am": Self-Presentation Preferences of People with Invisible Disabilities through Embodied Social VR Avatars</a>
                </div>
                <div class="pub-authors">Ria J. Gualano*, Lucy Jiang*, <b>Kexin Zhang</b>, Tanisha Shende, Andrea Stevenson Won, Shiri Azenkot</div>
                <div class="pub-venue">ASSETS 2024</div>
                <div class="pub-links">
                    <a href="#" class="abs-button">ABS</a>
                    <a href="https://dl.acm.org/doi/10.1145/3663548.3675620" target="_blank">DOI</a>
                    <a href="https://dl.acm.org/doi/pdf/10.1145/3663548.3675620" target="_blank">PDF</a>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3614480" target="_blank">POSTER</a>
                </div>
                <div class="pub-abstract">
                    With the increasing adoption of social virtual reality (VR), it is critical to design inclusive avatars. While researchers have investigated how and why blind and d/Deaf people wish to disclose their disabilities in VR, little is known about the preferences of many others with invisible disabilities (e.g., ADHD, dyslexia, chronic conditions). We filled this gap by interviewing 15 participants, each with one to three invisible disabilities, who represented 22 different invisible disabilities in total. We found that invisibly disabled people approached avatar-based disclosure through contextualized considerations informed by their prior experiences. For example, some wished to use VR's embodied affordances, such as facial expressions and body language, to dynamically represent their energy level or willingness to engage with others, while others preferred not to disclose their disability identity in any context. We define a binary framework for embodied invisible disability expression (public and private) and discuss three disclosure patterns (Activists, Non-Disclosers, and Situational Disclosers) to inform the design of future inclusive VR experiences.
                </div>
            </div>
        </div>

        <div class="pub-item">
            <img src="images/assets'23.png" alt="ASSETS 2023" class="pub-image">
            <div class="pub-item-content">
                <div class="pub-title">
                    <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388" target="_blank">A Diary Study in Social Virtual Reality: Impact of Avatars with Disability Signifiers on the Social Experiences of People with Disabilities</a>
                </div>
                <div class="pub-authors"><b>Kexin Zhang</b>, Elmira Deldari, Yaxing Yao, Yuhang Zhao</div>
                <div class="pub-venue">ASSETS 2023</div>
                <div class="pub-links">
                    <a href="#" class="abs-button">ABS</a>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388" target="_blank">DOI</a>
                    <a href="https://dl.acm.org/doi/epdf/10.1145/3597638.3608388" target="_blank">PDF</a>
                </div>
                <div class="pub-abstract">
                    People with disabilities (PWD) have shown a growing presence in the emerging social virtual reality (VR). To support disability representation, some social VR platforms start to involve disability features in avatar design. However, it is unclear how disability disclosure via avatars (and the way to present it) would affect PWD's social experiences and interaction dynamics with others. To fill this gap, we conducted a diary study with 10 PWD who freely explored VRChatâ€”a popular commercial social VR platformâ€”for two weeks, comparing their experiences between using regular avatars and avatars with disability signifiers (i.e., avatar features that indicate the user's disability in real life). We found that PWD preferred using avatars with disability signifiers and wanted to further enhance their aesthetics and interactivity. However, such avatars also caused embodied, explicit harassment targeting PWD. We revealed the unique factors that led to such harassment and derived design implications and protection mechanisms to inspire more safe and inclusive social VR.
                </div>
            </div>
        </div>

        <div class="pub-item">
            <img src="images/assets22.png" alt="ASSETS 2023" class="pub-image">
            <div class="pub-item-content">
                <div class="pub-title">
                    <a href="https://dl.acm.org/doi/abs/10.1145/3517428.3544829" target="_blank">"Itâ€™s Just Part of Me: Understanding Avatar Diversity andSelf-presentation of People with Disabilities in Social VirtualReality</a>
                </div>
                <div class="pub-authors"><b>Kexin Zhang</b>, Elmira Deldari, Zhicong Lu, Yaxing Yao, Yuhang Zhao</div>
                <div class="pub-venue">ASSETS 2022</div>
                <div class="pub-links">
                    <a href="#" class="abs-button">ABS</a>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3597638.3608388" target="_blank">DOI</a>
                    <a href="https://dl.acm.org/doi/epdf/10.1145/3517428.3544829" target="_blank">PDF</a>
                </div>
                <div class="pub-abstract">
                    In social Virtual Reality (VR), users are embodied in avatars and interact with other users in a face-to-face manner using avatars as the medium. With the advent of social VR, people with disabilities (PWD) have shown an increasing presence on this new social media. With their unique disability identity, it is not clear how PWD perceive their avatars and whether and how they prefer to disclose their disability when presenting themselves in social VR. We fill this gap by exploring PWD's avatar perception and disability disclosure preferences in social VR. Our study involved two steps. We first conducted a systematic review of fifteen popular social VR applications to evaluate their avatar diversity and accessibility support. We then conducted an in-depth interview study with 19 participants who had different disabilities to understand their avatar experiences. Our research revealed a number of disability disclosure preferences and strategies adopted by PWD (e.g., reflect selective disabilities, present a capable self). We also identified several challenges faced by PWD during their avatar customization process. We discuss the design implications to promote avatar accessibility and diversity for future social VR platforms.
                </div>
            </div>
        </div>

    </section>
</main>

<footer>
    &copy; 2025 Your Name. Built with Inter.
</footer>

<script>
    document.addEventListener('DOMContentLoaded', function () {
        // Handle abstract toggle
        const absButtons = document.querySelectorAll('.abs-button');
        absButtons.forEach(button => {
            button.addEventListener('click', function (event) {
                event.preventDefault();
                const abstract = this.closest('.pub-item-content').querySelector('.pub-abstract');
                if (abstract) {
                    abstract.classList.toggle('show');
                }
            });
        });
        
        // Handle smooth scrolling for navigation links
        const navLinks = document.querySelectorAll('.nav-link');
        navLinks.forEach(link => {
            link.addEventListener('click', function (event) {
                event.preventDefault();
                const targetId = this.getAttribute('href');
                
                if (targetId === '#about') {
                    // Scroll to top
                    window.scrollTo({
                        top: 0,
                        behavior: 'smooth'
                    });
                } else if (targetId === '#publications') {
                    // Scroll to publications section with offset for sticky header
                    const publicationsSection = document.getElementById('publications');
                    if (publicationsSection) {
                        const headerHeight = document.querySelector('header').offsetHeight;
                        const elementPosition = publicationsSection.getBoundingClientRect().top;
                        const offsetPosition = elementPosition + window.pageYOffset - headerHeight - 20;
                        
                        window.scrollTo({
                            top: offsetPosition,
                            behavior: 'smooth'
                        });
                    }
                }
            });
        });
    });
</script>

</body>
</html>